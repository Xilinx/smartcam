{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Xilinx Logo](images/xilinx_logo.png \"Xilinx Logo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction:\n",
    "\n",
    "This notebook demonstrates how to captures images from the MIPI devices, then performs face detection or refinedet or SSD inference on it with DPU, and send the video frames with bounding boxes of detected results either to DP to display or stream it out as an RTSP server.\n",
    "\n",
    "The application is based on the VVAS (Vitis Video Analytics SDK) framework, also utilizing the open source GStreamer plugins.\n",
    "\n",
    "Vitis Video Analytics SDK (VVAS) is developed by Xilinx to provide many useful GStreamer plugins as the middleware between the application and underlying FPGA acclerators, including DPU AI inference engine, and other PL accelerators such as the one for AI input preprocessing.\n",
    "\n",
    "Please refer to the [Kria™ KV260 Vision AI Starter Kit Applications GitHub Pages](https://xilinx.github.io/kria-apps-docs/index.html) for detailed HW/SW architecture and [Vitis Video Analytics SDK GitHub Pages](https://xilinx.github.io/VVAS/#) for the VVAS related info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Some User Options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Option to choose the AI model to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aitask=\"facedetect\" # \"refinedet\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Option to set input type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"mipi\" # choose either 'mipi' or 'usb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Option to choose DP / RTSP output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DP_output=True # True to choose DP output, False to choose RTSP output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Imports and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preapare Data to Visualize the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a directory for saving the pipeline graph as dot file. Set the GStreamer debug dot directory environment variable to point to that directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=\"smartcam\"\n",
    "dotdir = \"/tmp/gst-dot/\" + nb + \"/\"\n",
    "!mkdir -p $dotdir\n",
    "%env GST_DEBUG_DUMP_DOT_DIR = $dotdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all python modules required for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import system, util modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import subprocess\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add some util path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathv=\"{}:/usr/sbin:/sbin\".format(os.getenv(\"PATH\"))\n",
    "%env PATH = $pathv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GStreamer related library import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gi\n",
    "gi.require_version('Gst', '1.0')\n",
    "gi.require_version(\"GstApp\", \"1.0\")\n",
    "gi.require_version('GstVideo', '1.0')\n",
    "gi.require_version('GstRtspServer', '1.0')\n",
    "gi.require_version('GIRepository', '2.0')\n",
    "from gi.repository import GObject, GLib, Gst, GstVideo, GstRtspServer, GLib, GIRepository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gst.init(None)\n",
    "#Gst.debug_set_threshold_from_string('*:1', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Construct the String Representation of GStreamer Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function ***get_media_by_device*** \n",
    "\n",
    "This function returns the matching media node for a given video capture source.\n",
    "\n",
    "The following sources are supported in this notebook:\n",
    "\n",
    "* usb : requires USB webcam supporting 1080p output, we recommend the [Logitech BRIO](https://www.logitech.com/en-in/products/webcams/brio-4k-hdr-webcam.960-001105.html).\n",
    "* mipi : platform1 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_media_dev_by_name(src):\n",
    "    sources = {\n",
    "        \"usb\" : 'uvcvideo',\n",
    "        'mipi' : 'vcap_csi',\n",
    "    }\n",
    "    devices = glob.glob('/dev/media*')\n",
    "    for dev in devices:\n",
    "        proc = subprocess.run(['media-ctl', '-d', dev, '-p'], capture_output=True, encoding='utf8')\n",
    "        for line in proc.stdout.splitlines():\n",
    "            if sources[src] in line:\n",
    "                return dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_dev_of_mediadev(src):\n",
    "    proc = subprocess.Popen(['media-ctl', '-d', src, '-p'], stdout=subprocess.PIPE)\n",
    "    output = subprocess.check_output(('awk', '/^driver\\s*uvcvideo/ {u=1} /device node name *\\/dev\\/video/ {x=$4;f=1;next} u&&f&&/pad0: Sink/ {print x; x=\"\"} f {f=0}'), stdin=proc.stdout).decode('utf8').splitlines()\n",
    "    if len(output) > 1:\n",
    "        return output[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the mediasrc\n",
    "\n",
    "* Get the mediasrc index by calling get_media_dev_by_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_device = get_media_dev_by_name(source) \n",
    "if media_device is None:\n",
    "    raise Exception('Unable to find video source ' + source + '. Make sure the device is plugged in, powered, and the correct platform is used.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* mediasrcbin is the Xilinx developed plugin for media devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if source == \"mipi\":\n",
    "    src = \"mediasrcbin media-device=\" + media_device\n",
    "    if DP_output:\n",
    "        src += \" v4l2src0::io-mode=dmabuf v4l2src0::stride-align=256 \"\n",
    "elif source == \"usb\":\n",
    "    usbmedia=media_device\n",
    "    usbvideo=get_video_dev_of_mediadev(usbmedia)\n",
    "    src = \"v4l2src name=videosrc device={usbvideo} io-mode=mmap stride-align=256 \".format(usbvideo=usbvideo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the real pipeline string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configuration directory for IVAS plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confdir=\"/opt/xilinx/share/ivas/smartcam/\"+aitask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the caps.\n",
    "User can change the resolution and framerate here.\n",
    "\n",
    "If videosrc cannot support format NV12, adjust the pipeline to fit with followning elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if source==\"mipi\":\n",
    "    pip=src + ' ! video/x-raw, width=1920, height=1080, format=NV12, framerate=30/1 '\n",
    "elif source==\"usb\":\n",
    "    pip=src + ' ! video/x-raw, width=1920, height=1080 ! videoconvert ! video/x-raw, format=NV12 '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add one branch to perform AI inference.\n",
    "\n",
    "* ivas_xmultisrc kconfig=\"{confdir}/preprocess.json\"\n",
    "\n",
    "    This is for an element to do colorspace conversion from NV12 to BGR, scale to the size needed by DPU, and also perform the quantization as DPU model needed.\n",
    "    \n",
    "    In current project there's a dedicate PL ***accelerator pp_pipeline_accel:pp_pipeline_accel_1@0xa0020000*** will do this work with greater performance thatn software version.\n",
    "\n",
    "    Detailed configuration please see the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip += ' ! tee name=t  ! queue \\\n",
    "! ivas_xmultisrc kconfig=\"{confdir}/preprocess.json\" '.format(confdir=confdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AI inference\n",
    "\n",
    "    With the buffer being preprocess, AI inference plugin is linked to perform the AI tasks.\n",
    "    \n",
    "    The DPU AI inference hardware engine and Vitis AI library is behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip += ' ! queue ! ivas_xfilter kernels-config=\"{confdir}/aiinference.json\" '.format(confdir=confdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AI inference meta pass down\n",
    "\n",
    "    AI inference results is passed to sink_master pad of ***ivas_xmetaaffixer***, which is an IVAS plugin which can scale the meta info, such as bbox info, based on the size ratio of the buffers of sink_slave to sink_master.\n",
    "    \n",
    "    For detailed usage please refer to IVAS docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip += ' ! ima.sink_master ivas_xmetaaffixer name=ima ima.src_master ! fakesink '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another branch to accept the inference meta data, and drawing boundingbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accept and scale the original AI inference meta info\n",
    "\n",
    "    As the previous step, the meta info is pass down to here, the original buffer from **t.** is linked to the sink_slave_0, and get the scaled meta at the corresponding src_slave_0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip += ' t. ! queue max-size-buffers=1 leaky=2 ! ima.sink_slave_0 ima.src_slave_0 '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Draw bbox on the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip += ' ! queue ! ivas_xfilter kernels-config=\"{confdir}/drawresult.json\" '.format(confdir=confdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Two types of Outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DP output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DP_output:\n",
    "    pip += ' ! queue ! kmssink driver-name=xlnx plane-id=39 sync=false fullscreen-overlay=true '\n",
    "    pipe = Gst.parse_launch(pip)\n",
    "    pipe.set_state(Gst.State.PLAYING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the GStreamer Pipeline Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate pipeline dot file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DP_output:\n",
    "    Gst.debug_bin_to_dot_file(pipe, Gst.DebugGraphDetails.ALL, nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convert the dot file to png and display the pipeline graph.\n",
    "\n",
    "    The image will be displayed bellow the following code cell.\n",
    "\n",
    "    **Note**: This step may take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "from IPython.display import Image, display, clear_output\n",
    "if DP_output:\n",
    "    dotfile = dotdir + \"/\" + nb + \".dot\"\n",
    "    print(\"Converting dot to graph...\")\n",
    "    graph = pydot.graph_from_dot_file(dotfile, 'utf-8')\n",
    "    display(Image(graph[0].create(None,'png', 'utf-8')))\n",
    "    print(\"Pipeline graph is shown, double click it to zoom in and out.\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mainloop to be interruptable by clicking the stop square button on the Jupyter toolbar.\n",
    "\n",
    "  **Notice:** For DP output case, stopping the process with the square button can only work until the previous step finishes to show the pipeline dot graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DP_output:\n",
    "    loop = GLib.MainLoop()\n",
    "    try:\n",
    "        loop.run()\n",
    "    except:\n",
    "        sys.stdout.write(\"Interrupt caught\\n\")\n",
    "        Gst.debug_bin_to_dot_file(pipe, Gst.DebugGraphDetails.ALL, nb)\n",
    "        pipe.set_state(Gst.State.NULL)\n",
    "        loop.quit()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RTSP output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Start an RTSP server will consume the GStreamer pipeline constructed in next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DP_output:\n",
    "    mainloop = GLib.MainLoop()\n",
    "    server = GstRtspServer.RTSPServer.new()\n",
    "    server.props.service = \"5000\"\n",
    "    mounts = server.get_mount_points()\n",
    "    serverid=server.attach(None)\n",
    "    factory = GstRtspServer.RTSPMediaFactory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then pass the frame with bbox to do VCU encoding with bbox info as encoding ROI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ROI info for VCU encoding generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DP_output:\n",
    "    pip += ' ! queue ! ivas_xroigen roi-type=1 roi-qp-delta=-10 roi-max-num=10   '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* VCU encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DP_output:\n",
    "    pip += '! queue ! omxh264enc qp-mode=1 num-slices=8 gop-length=60 \\\n",
    " periodicity-idr=270 control-rate=low-latency \\\n",
    " gop-mode=low-delay-p gdr-mode=horizontal cpb-size=200 \\\n",
    " initial-delay=100  filler-data=false min-qp=15 \\\n",
    " max-qp=40  b-frames=0  low-bandwidth=false  target-bitrate=3000 \\\n",
    "! video/x-h264, alignment=au '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RTP payloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DP_output:\n",
    "    pip += '! queue ! rtph264pay name=pay0 pt=96'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Start the RTSP Server With the Pipeline String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DP_output:\n",
    "    factory.set_launch('( ' + pip + ' )')\n",
    "    factory.set_shared(True)\n",
    "    mounts.add_factory(\"/test\", factory)\n",
    "\n",
    "    out=subprocess.check_output(\"ifconfig | grep inet\", shell=True)\n",
    "    for line in out.decode(\"ascii\").splitlines():\n",
    "        m = re.search('inet *(.*?) ', line)\n",
    "        if m:\n",
    "            found = m.group(1)\n",
    "            if found != \"127.0.0.1\":\n",
    "                break\n",
    "    uri=\"rtsp://{}:{}/test\".format(\"127.0.0.1\" if (found==\"\") else found, server.props.service)\n",
    "    print (\"Video is now streaming from {src} source. \\n\\\n",
    "    Run the command \\\"ffplay {uri}\\\" in another PC which have network access to the SoM board to view the video.\\n\".format(src=source, uri=uri))\n",
    "    try:\n",
    "        mainloop.run()\n",
    "    except:\n",
    "        sys.stdout.write(\"Interrupt caught.\\n\")\n",
    "        GLib.Source.remove(serverid)\n",
    "        mainloop.quit()\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Summary\n",
    "The Jupyter application shows how to:\n",
    "\n",
    "1. Create a GStreamer pipeline which utilize the VVAS framework to call Vitis AI Library to do face detection inference on the incoming frames, and draw boundboxing of detected results.\n",
    "2. Use the GStreamer RTSPServer module to setup an RTSP server.\n",
    "3. User can try to customize the source to video file or USB camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Copyright© 2021 Xilinx</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
